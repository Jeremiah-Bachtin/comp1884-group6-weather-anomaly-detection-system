{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:55:23.872685Z",
     "start_time": "2025-06-05T14:55:23.867161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from utils.find_root import find_project_root\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib"
   ],
   "id": "3a9af6eeac528718",
   "outputs": [],
   "execution_count": 466
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:55:23.904260Z",
     "start_time": "2025-06-05T14:55:23.893732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve the project root dynamically and set it as working directory\n",
    "project_root = find_project_root()\n",
    "os.chdir(project_root)\n",
    "\n",
    "# Define dashboard input directory\n",
    "INFERENCE_OUTPUT_DIR = \"outputs/modelling/inference/\"\n",
    "\n",
    "# Ensure output directory exist\n",
    "os.makedirs(INFERENCE_OUTPUT_DIR, exist_ok=True)"
   ],
   "id": "45df7401a9a6dfd1",
   "outputs": [],
   "execution_count": 467
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:55:24.108926Z",
     "start_time": "2025-06-05T14:55:24.032138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pre-ingested slices\n",
    "df_hist = pd.read_csv(\"data/processed/rolling_window/baseline_rolling_1440h_until_20250531_1600.csv\", parse_dates=[\"date\"], index_col='date')\n",
    "df_fcst = pd.read_csv(\"data/raw/forecast/forecast_72h_from_20250531_1700.csv\", parse_dates=[\"date\"], index_col='date')\n",
    "# Set dates to timezone-naive\n",
    "df_hist.index = df_hist.index.tz_localize(None)\n",
    "df_fcst.index = df_fcst.index.tz_localize(None)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 468
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 1 – Preprocess `df_hist` and Apply Frozen Stats to the 72-Hour Forecast\n",
    "\n",
    "**Goal**:\n",
    "Prepare historical weather data (`df_hist`) for aligning the upcoming 72-hour forecast (`df_fcst`) with the anomaly detection models. This includes computing log transforms, smoothing, and freezing statistical baselines for scaling and dashboard banding — all based on the most recent 60 days of observations.\n",
    "\n",
    "---\n",
    "\n",
    "**Rationale**:\n",
    "To maintain fair and realistic inference conditions, all transformations applied to the forecast must rely solely on historical statistics. This ensures the model never \"peeks\" at future data — a principle known as **temporal separation**, which is essential for avoiding data leakage and preserving the integrity of time-series anomaly detection (Wenig, 2022; Trinh, 2022).\n",
    "\n",
    "Instead of applying rolling windows to the forecast (which would require future values), we freeze the statistical baselines (mean, std, IQR, quantiles) using the latest 1440 hours of clean historical data. These \"frozen stats\" are then broadcasted across all 72 forecast hours to ensure consistency in scoring and visual interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "**Visual Banding Strategy**:\n",
    "We also define \"normality bounds\" for the dashboard, using feature-specific approaches suited to their statistical behaviour:\n",
    "\n",
    "- **Temperature**:\n",
    "  We apply `Q1` as the lower bound and `Q3 + 1.5 × IQR` as the upper bound to capture typical variation while filtering out seasonal extremes. This is a robust and interpretable method used widely in anomaly labelling (Wenig, 2022).\n",
    "\n",
    "- **Wind Speed**:\n",
    "  We use the **10th percentile** as the lower bound and `Q3 + 1.5 × IQR` for the upper, acknowledging the strong right skew in wind behaviour and to avoid overstating calm conditions (Tawalkuli, 2024).\n",
    "\n",
    "- **Surface Pressure**:\n",
    "  Since pressure is generally well-behaved and symmetric, we use `mean ± 2 × std`, a conventional method aligned with Gaussian expectations (Darban, 2024).\n",
    "\n",
    "- **Precipitation**:\n",
    "  Precipitation is highly skewed with frequent zeros, so we adopt a threshold strategy: 0.5mm (Light Rain), 2mm (ModerateRain) and 5mm (Heavy Rain) arbitrarily set and implemented in the dashboard code (not at inference).\n",
    "\n",
    "---\n",
    "\n",
    "All forecast rows now receive identical reference bands and scaling baselines — ensuring clarity, integrity, and robustness in downstream model scoring and dashboard display."
   ],
   "id": "668ca0960d744f8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:55:24.391748Z",
     "start_time": "2025-06-05T14:55:24.346947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1 – Preprocess df_hist and prepare df_fcst for inference\n",
    "\n",
    "eps = 1e-6\n",
    "window_3h, min_3h = 3, 1\n",
    "\n",
    "# ---- Step 1.1: Preprocess df_hist (frozen 1440h) ----\n",
    "\n",
    "# Compute log1p of raw precipitation\n",
    "df_hist[\"precip_log\"] = np.log1p(df_hist[\"precipitation\"])\n",
    "\n",
    "# Smooth wind speed (for wind_r scaling)\n",
    "df_hist[\"wind_smoothed\"] = df_hist[\"wind_speed_10m\"].rolling(window_3h, min_periods=min_3h).mean()\n",
    "\n",
    "# ---- Step 1.2: Extract frozen stats for forecast z-scoring ----\n",
    "stats_at_tnow = {\n",
    "    # Temperature\n",
    "    \"temp_mean\": df_hist[\"temperature_2m\"].mean(),\n",
    "    \"temp_std\": df_hist[\"temperature_2m\"].std() + eps,\n",
    "\n",
    "    # Surface pressure\n",
    "    \"press_mean\": df_hist[\"surface_pressure\"].mean(),\n",
    "    \"press_std\": df_hist[\"surface_pressure\"].std() + eps,\n",
    "\n",
    "    # Wind (smoothed)\n",
    "    \"wind_median\": df_hist[\"wind_smoothed\"].median(),\n",
    "    \"wind_q1\": df_hist[\"wind_smoothed\"].quantile(0.25),\n",
    "    \"wind_q3\": df_hist[\"wind_smoothed\"].quantile(0.75),\n",
    "    \"wind_iqr\": df_hist[\"wind_smoothed\"].quantile(0.75) - df_hist[\"wind_smoothed\"].quantile(0.25) + eps,\n",
    "\n",
    "    # Precipitation (log1p)\n",
    "    \"precip_log_mean_12h\": df_hist[\"precip_log\"].iloc[-12:].mean(),\n",
    "    \"precip_log_std_12h\": df_hist[\"precip_log\"].iloc[-12:].std() + eps,\n",
    "    \"precip_log_mean_24h\": df_hist[\"precip_log\"].iloc[-24:].mean(),\n",
    "    \"precip_log_std_24h\": df_hist[\"precip_log\"].iloc[-24:].std() + eps\n",
    "}\n",
    "\n",
    "# ---- Step 1.3: Compute dashboard visualisation bounds ----\n",
    "\n",
    "# Temperature\n",
    "temp_q1 = df_hist[\"temperature_2m\"].quantile(0.25)\n",
    "temp_q3 = df_hist[\"temperature_2m\"].quantile(0.75)\n",
    "temp_upper = temp_q3 + 1.5 * (temp_q3 - temp_q1)\n",
    "\n",
    "# Wind speed (raw)\n",
    "wind_q1 = df_hist[\"wind_speed_10m\"].quantile(0.25)\n",
    "wind_q3 = df_hist[\"wind_speed_10m\"].quantile(0.75)\n",
    "wind_p10 = df_hist[\"wind_speed_10m\"].quantile(0.10)\n",
    "wind_upper = wind_q3 + 1.5 * (wind_q3 - wind_q1)\n",
    "\n",
    "# Pressure (normal assumption)\n",
    "press_mean = stats_at_tnow[\"press_mean\"]\n",
    "press_std = stats_at_tnow[\"press_std\"]\n",
    "press_lower = press_mean - 2 * press_std\n",
    "press_upper = press_mean + 2 * press_std\n",
    "\n",
    "# ---- Step 1.4: Apply transformations and broadcast bounds to df_fcst ----\n",
    "\n",
    "# Forecast-time log1p transform\n",
    "df_fcst[\"precip_log\"] = np.log1p(df_fcst[\"precipitation\"])\n",
    "\n",
    "# Apply frozen z-scores and scaled features\n",
    "df_fcst[\"temperature_2m_z\"] = (df_fcst[\"temperature_2m\"] - stats_at_tnow[\"temp_mean\"]) / stats_at_tnow[\"temp_std\"]\n",
    "df_fcst[\"surface_pressure_z\"] = (df_fcst[\"surface_pressure\"] - stats_at_tnow[\"press_mean\"]) / stats_at_tnow[\"press_std\"]\n",
    "\n",
    "# Wind smoothed and robust scaled\n",
    "df_fcst[\"wind_smoothed\"] = df_fcst[\"wind_speed_10m\"].rolling(window_3h, min_periods=min_3h).mean()\n",
    "df_fcst[\"wind_r\"] = (df_fcst[\"wind_smoothed\"] - stats_at_tnow[\"wind_median\"]) / stats_at_tnow[\"wind_iqr\"]\n",
    "\n",
    "# Precip log z-scores\n",
    "df_fcst[\"precip_z_12h\"] = (df_fcst[\"precip_log\"] - stats_at_tnow[\"precip_log_mean_12h\"]) / stats_at_tnow[\"precip_log_std_12h\"]\n",
    "df_fcst[\"precip_z_24h\"] = (df_fcst[\"precip_log\"] - stats_at_tnow[\"precip_log_mean_24h\"]) / stats_at_tnow[\"precip_log_std_24h\"]\n",
    "\n",
    "# Broadcast dashboard bounds (same value across all forecast hours)\n",
    "df_fcst[\"temp_lower\"] = temp_q1\n",
    "df_fcst[\"temp_upper\"] = temp_upper\n",
    "\n",
    "df_fcst[\"wind_lower\"] = wind_p10\n",
    "df_fcst[\"wind_upper\"] = wind_upper\n",
    "\n",
    "df_fcst[\"press_lower\"] = press_lower\n",
    "df_fcst[\"press_upper\"] = press_upper\n",
    "\n",
    "# ---- Step 1.5 – Combine Preprocessed Historical and Forecast Data ----\n",
    "# Ensure aligned structure and sort order before stitching\n",
    "df_combined_ready = pd.concat([df_hist, df_fcst])\n",
    "# Done — df_combined_ready is now the unified, model-aligned dataset for IF and LSTM-AE"
   ],
   "id": "10ec0b117094a9b7",
   "outputs": [],
   "execution_count": 469
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 2 – LSTM Data Preprocessing\n",
    "\n",
    "**Goal**: Prepare the unified dataset (`df_combined_ready`) for LSTM-AE inference by replicating the preprocessing steps applied during training.\n",
    "\n",
    "This includes:\n",
    "\n",
    "- **Cyclical feature engineering** to capture the periodic structure of time (month and hour),\n",
    "- **Sliding sequence construction** using 720-hour windows with stride 1, covering both historical and forecast periods,\n",
    "- **Per-sequence robust scaling** (median and IQR) of core weather features to ensure numerical stability and alignment with training-time expectations.\n",
    "\n",
    "> Note: `precip_log` is excluded from robust scaling due to its high sparsity and non-normal distribution (Tawalkuli, 2024). Instead, it is included in raw form within each sequence as per the modelling logic."
   ],
   "id": "6b088ad69a6ba445"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:55:24.518600Z",
     "start_time": "2025-06-05T14:55:24.488020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2.1 – Add cyclical time features to df_combined_ready\n",
    "df_combined_ready[\"hour\"] = df_combined_ready.index.hour\n",
    "df_combined_ready[\"month\"] = df_combined_ready.index.month\n",
    "\n",
    "# Step 2.2 Encode cyclical time features using sine and cosine\n",
    "df_combined_ready[\"hour_sin\"] = np.sin(2 * np.pi * df_combined_ready[\"hour\"] / 24)\n",
    "df_combined_ready[\"hour_cos\"] = np.cos(2 * np.pi * df_combined_ready[\"hour\"] / 24)\n",
    "df_combined_ready[\"month_sin\"] = np.sin(2 * np.pi * df_combined_ready[\"month\"] / 12)\n",
    "df_combined_ready[\"month_cos\"] = np.cos(2 * np.pi * df_combined_ready[\"month\"] / 12)"
   ],
   "id": "b145288f4bb9511a",
   "outputs": [],
   "execution_count": 470
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:55:25.291031Z",
     "start_time": "2025-06-05T14:55:24.613434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2.3 – Construct 720-hour LSTM-AE sequences with per-sequence robust scaling\n",
    "\n",
    "# Define input features for LSTM-AE\n",
    "# Raw precipitation is excluded in favour of unscaled precip_log\n",
    "lstm_features = [\n",
    "    \"temperature_2m\", \"surface_pressure\", \"wind_speed_10m\",\n",
    "    \"precip_log\",  # retained in raw log-transformed form\n",
    "    \"hour_sin\", \"hour_cos\", \"month_sin\", \"month_cos\"\n",
    "]\n",
    "\n",
    "# Define which features will be robustly scaled (median + IQR)\n",
    "features_to_scale = [\"temperature_2m\", \"surface_pressure\", \"wind_speed_10m\"]\n",
    "feature_indices = [lstm_features.index(f) for f in features_to_scale]\n",
    "\n",
    "# Define a utility function to apply robust scaling per sequence\n",
    "def robust_scale_sequence(seq, indices, eps=1e-5):\n",
    "    seq = seq.copy()\n",
    "    for i in indices:\n",
    "        col = seq[:, i]\n",
    "        median = np.median(col)\n",
    "        iqr = np.percentile(col, 75) - np.percentile(col, 25)\n",
    "        seq[:, i] = (col - median) / (iqr + eps)\n",
    "    return seq\n",
    "\n",
    "# Set window and stride\n",
    "sequence_length = 720\n",
    "stride = 1\n",
    "\n",
    "# Extract raw data matrix from the combined DataFrame\n",
    "raw_array = df_combined_ready[lstm_features].values\n",
    "lstm_sequences = []\n",
    "sequence_end_times = []\n",
    "\n",
    "# Slide through the dataset with stride 1 to construct scaled sequences\n",
    "for start in range(0, len(df_combined_ready) - sequence_length + 1, stride):\n",
    "    seq = raw_array[start:start + sequence_length]\n",
    "    seq_scaled = robust_scale_sequence(seq, feature_indices)\n",
    "    lstm_sequences.append(seq_scaled)\n",
    "    sequence_end_times.append(df_combined_ready.index[start + sequence_length - 1])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "lstm_sequences = np.stack(lstm_sequences)  # shape: [num_sequences, 720, 8]\n",
    "sequence_end_times = pd.DatetimeIndex(sequence_end_times) # shape: [num_sequences]\n",
    "\n",
    "# Save sequences as a binary .npy file\n",
    "np.save(os.path.join(INFERENCE_OUTPUT_DIR,\"lstm_sequences.npy\"), lstm_sequences)\n",
    "\n",
    "# Save sequence end timestamps as a CSV\n",
    "sequence_end_times.to_series().to_csv(\n",
    "    os.path.join(INFERENCE_OUTPUT_DIR,\"sequence_end_times.csv\"),\n",
    "    header=[\"end_time\"],\n",
    "    index_label=\"sequence_idx\"\n",
    ")\n",
    "\n",
    "print(\"✅ LSTM sequences and end times saved to outputs/modelling/inference/\")"
   ],
   "id": "fa6bb0fe0f54d93a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LSTM sequences and end times saved to outputs/modelling/inference/\n"
     ]
    }
   ],
   "execution_count": 471
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3 – Score 72-hour Forecast Using Trained IF and LSTM-AE + Assign Anomaly Labels\n",
    "\n",
    "**Goal**: Apply both trained models to the most recent forecast window and generate timestamp-level anomaly labels. The model scores and assigned anomaly labels form the basis for interpreting anomalous behaviour over the next 72 hours.\n",
    "\n",
    "This step includes:\n",
    "\n",
    "- Use the trained Isolation Forest model to score the transformed 72-hour forecast slice. Apply the fixed threshold (determined during training) to label anomalies per hour.\n",
    "\n",
    "- Use the trained LSTM-AE to compute reconstruction errors across all 720-hour sequences. Each sequence ends with a timestamp, and the final timestep of each sequence corresponds to a forecast timestamp. We compute a per-timestamp error from the final step of each sequence, then apply the pre-established 95th percentile threshold to label anomalies.\n",
    "\n",
    "- Assign each timestamp an anomaly label based on whether it was classed as an anomaly by LSTM-AE | IF | both | none in accordance with the modelling logic."
   ],
   "id": "c63c27210ab3cf21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:55:25.559708Z",
     "start_time": "2025-06-05T14:55:25.483039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3.1 – Score the 72-hour forecast using trained Isolation Forest model\n",
    "\n",
    "# Load trained Isolation Forest model\n",
    "if_model = joblib.load(\"outputs/modelling/models/if_model.joblib\")\n",
    "\n",
    "# Define input features (same as training)\n",
    "if_features = [\"temperature_2m_z\", \"surface_pressure_z\", \"wind_r\", \"precip_z_12h\",\"precip_z_24h\"]\n",
    "\n",
    "# Extract forecast portion (last 72 rows)\n",
    "df_fcst_ready = df_combined_ready.iloc[-72:].copy()\n",
    "\n",
    "# Compute anomaly scores using decision_function (used during training)\n",
    "df_fcst_ready[\"if_score\"] = if_model.decision_function(df_fcst_ready[if_features])\n",
    "\n",
    "# Apply fixed anomaly threshold derived during training (example value shown)\n",
    "if_threshold = 0.03304385848702787 # ← Replace with actual saved threshold from training\n",
    "df_fcst_ready[\"is_if_anomaly\"] = (df_fcst_ready[\"if_score\"] < if_threshold).astype(int)"
   ],
   "id": "31ebd44656d1c2af",
   "outputs": [],
   "execution_count": 472
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:55:30.701822Z",
     "start_time": "2025-06-05T14:55:25.690139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3.2 – Compute LSTM-AE MAE per timestamp and flag anomalies using fixed threshold\n",
    "\n",
    "# Load trained LSTM Autoencoder model\n",
    "lstm_ae = load_model(\"outputs/modelling/models/lstm_ae_best.h5\")\n",
    "\n",
    "# Predict reconstructions from LSTM-AE\n",
    "lstm_recon = lstm_ae.predict(lstm_sequences, verbose=0)  # shape: [n_seq, 720, 8]\n",
    "\n",
    "# Compute MAE per timestep across all 8 LSTM input features\n",
    "mae_seq = np.mean(np.abs(lstm_sequences - lstm_recon), axis=2)  # shape: [n_seq, 720]\n",
    "\n",
    "# Build timestamp array per sequence\n",
    "timestamps_array = sequence_end_times.values[:, None] - np.arange(sequence_length - 1, -1, -1).astype(\"timedelta64[h]\")\n",
    "flat_timestamps = timestamps_array.ravel()\n",
    "flat_errors = mae_seq.ravel()\n",
    "\n",
    "# Assemble tidy per-timestamp error dataframe\n",
    "df_lstm_errors_forecast = pd.DataFrame({\n",
    "    \"date\": pd.to_datetime(flat_timestamps),\n",
    "    \"lstm_error\": flat_errors\n",
    "})\n",
    "\n",
    "# Aggregate overlapping timestamps by mean\n",
    "df_lstm_errors_forecast_agg = (\n",
    "    df_lstm_errors_forecast\n",
    "    .groupby(\"date\")[\"lstm_error\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Apply fixed threshold from retrospective scoring\n",
    "lstm_threshold = 0.6425  # ← Replace with actual threshold used in training\n",
    "df_lstm_errors_forecast_agg[\"is_lstm_anomaly\"] = (df_lstm_errors_forecast_agg[\"lstm_error\"] > lstm_threshold).astype(int)\n",
    "df_lstm_errors_forecast_agg[\"lstm_error\"].describe()"
   ],
   "id": "a880a4abdff2f3ea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1512.000000\n",
       "mean        0.424936\n",
       "std         0.106004\n",
       "min         0.170420\n",
       "25%         0.349350\n",
       "50%         0.412613\n",
       "75%         0.482797\n",
       "max         0.842720\n",
       "Name: lstm_error, dtype: float64"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 473
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:55:30.951705Z",
     "start_time": "2025-06-05T14:55:30.937631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3.3 – Safely merge LSTM-AE per-timestamp scores with forecast features\n",
    "\n",
    "df_fcst_ready = df_fcst_ready.merge(\n",
    "    df_lstm_errors_forecast_agg,\n",
    "    how=\"left\",\n",
    "    left_index=True,\n",
    "    right_on=\"date\"\n",
    ").set_index(\"date\")"
   ],
   "id": "cbf770854c7f846e",
   "outputs": [],
   "execution_count": 474
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:55:32.944807Z",
     "start_time": "2025-06-05T14:55:32.930258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3.4 – Broadcast fixed thresholds for visibility and debugging\n",
    "df_fcst_ready[\"if_threshold\"] = if_threshold\n",
    "df_fcst_ready[\"lstm_threshold\"] = lstm_threshold"
   ],
   "id": "8c287426b8eba042",
   "outputs": [],
   "execution_count": 475
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:55:33.213612Z",
     "start_time": "2025-06-05T14:55:33.198556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3.5 – Assign anomaly labels based on hybrid model outputs\n",
    "\n",
    "def assign_anomaly_label(row):\n",
    "    if row[\"is_if_anomaly\"] == 1 and row[\"is_lstm_anomaly\"] == 1:\n",
    "        return \"Compound anomaly\"\n",
    "    elif row[\"is_if_anomaly\"] == 1:\n",
    "        return \"Point anomaly\"\n",
    "    elif row[\"is_lstm_anomaly\"] == 1:\n",
    "        return \"Pattern anomaly\"\n",
    "    else:\n",
    "        return \"Normal\"\n",
    "\n",
    "# Apply to forecast-ready dataframe\n",
    "df_fcst_ready[\"anomaly_label\"] = df_fcst_ready.apply(assign_anomaly_label, axis=1)"
   ],
   "id": "366fb8e96c6cffdf",
   "outputs": [],
   "execution_count": 476
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:55:33.419896Z",
     "start_time": "2025-06-05T14:55:33.405867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count occurrences\n",
    "value_counts = df_fcst_ready[\"anomaly_label\"].value_counts()\n",
    "\n",
    "# Compute proportions\n",
    "proportions = df_fcst_ready[\"anomaly_label\"].value_counts(normalize=True) * 100  # Convert to percentage\n",
    "\n",
    "# Combine into a DataFrame\n",
    "summary_df = pd.DataFrame({\"Count\": value_counts, \"Proportion (%)\": proportions})\n",
    "\n",
    "print(summary_df)"
   ],
   "id": "dc36baa7b1058d09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Count  Proportion (%)\n",
      "anomaly_label                          \n",
      "Normal               58       80.555556\n",
      "Compound anomaly      9       12.500000\n",
      "Pattern anomaly       5        6.944444\n"
     ]
    }
   ],
   "execution_count": 477
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:55:50.111646Z",
     "start_time": "2025-06-05T14:55:50.101118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subset = ['temperature_2m', 'surface_pressure', 'precipitation', 'wind_speed_10m','temp_lower', 'temp_upper',\n",
    "       'wind_lower', 'wind_upper', 'press_lower', 'press_upper','if_score', 'is_if_anomaly', 'lstm_error',\n",
    "       'is_lstm_anomaly', 'if_threshold', 'lstm_threshold', 'anomaly_label']\n",
    "df_dashboard_ready = df_fcst_ready[subset]"
   ],
   "id": "267d64f6515bf784",
   "outputs": [],
   "execution_count": 479
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:55:52.001778Z",
     "start_time": "2025-06-05T14:55:51.978207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract forecast timestamp from df_fcst filename\n",
    "fcst_filename = os.path.basename(\"data/raw/forecast/forecast_72h_from_20250531_1700.csv\")\n",
    "fcst_tag = fcst_filename.replace(\"forecast_72h_from_\", \"\").replace(\".csv\", \"\")\n",
    "\n",
    "# Define output file paths\n",
    "inference_output_path = os.path.join(INFERENCE_OUTPUT_DIR, f\"inference_{fcst_tag}.csv\")\n",
    "dashboard_output_path = os.path.join(INFERENCE_OUTPUT_DIR, f\"dashboard_input_{fcst_tag}.csv\")\n",
    "\n",
    "# Save outputs\n",
    "df_fcst_ready.to_csv(inference_output_path, index=True)\n",
    "df_dashboard_ready.to_csv(dashboard_output_path, index=True)"
   ],
   "id": "5139c354263e8957",
   "outputs": [],
   "execution_count": 480
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "63ed574caf5f67ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "68490dce92f2e8a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
